{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import unicodedata\n","\n","import sys\n","import re\n","from os import path\n","\n","sys.path.append(\"../../../src\")\n","sys.path.append(\"../../../scripts\")\n","from inception_fishing import Annotation\n","\n","from tqdm import tqdm\n","\n","\n","from importlib import reload"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from s2_prepare_articles import *\n","import spacy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","from spatial_articles_utils import *\n","\n","# if updating spatial_articles_utils, run this code:\n","%run -m spatial_articles_utils"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["spacy_tokenizer = spacy.load(\"fr_core_news_sm\")"]},{"cell_type":"markdown","metadata":{},"source":["# Documents' text preparation\n","## Normalizing texts and investigating text issues"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Evaluating normalized text difference with original text:\n","\n","polities_dtf[\"normalized_text\"] = polities_dtf.document.apply(lambda d: unicodedata.normalize(\"NFKC\",d.text))\n","\n","polities_dtf[\"len_normalized_text\"] = polities_dtf[\"normalized_text\"].apply(len)\n","polities_dtf[\"len_unnormalized_text\"] = polities_dtf.document.apply(lambda d: len(d.text))\n","polities_dtf[\"len_diff_normalized_text\"] = polities_dtf[\"len_unnormalized_text\"] - polities_dtf[\"len_normalized_text\"]\n","polities_dtf[\"len_diff_normalized_text\"].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Character normalization investigation -> NFKC is the way to go :-)\n","\n","# Zoug had a very big 2529 char diff using NFKD\n","lendif2529NFKD = polities_dtf[polities_dtf[\"hds_article_id\"]==\"007373\"]\n","\n","def investigate_norm_len_diff(dtf, i=0):\n","    norm_text = dtf[\"normalized_text\"].iloc[i]\n","    unnorm_text = dtf.document.iloc[i].text\n","\n","    norm_chars = set(norm_text)\n","    unnorm_chars = set(unnorm_text)\n","\n","    unnorm_missing_chars = [ c for c in unnorm_chars if c not in norm_chars]\n","    norm_missing_chars = [ c for c in norm_chars if c not in unnorm_chars]\n","\n","    #return (norm_text, unnorm_text, norm_chars, unnorm_chars, norm_missing_chars, unnorm_missing_chars)\n","    return (norm_missing_chars, unnorm_missing_chars)\n","\n","investigate_norm_len_diff(lendif2529NFKD)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lendifNFKC = polities_dtf[polities_dtf[\"len_diff_normalized_text\"]!=0]\n","\n","\"\"\"Still some problem with some minor characters... we'll come to it when we need ta\"\"\"\n","polities_dtf.loc[polities_dtf.polity_id.apply(lambda i: i in [\"001256-c\", \"001321-c\", \"007384-ct\"]),:]\n","\n","\n","[investigate_norm_len_diff(lendifNFKC, i) for i in range(lendifNFKC.shape[0])]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["grandson_dtf = polities_dtf[polities_dtf.toponym==\"Grandson\"]\n","grandson_article = grandson_dtf.article.iloc[0]\n","grandson_document = grandson_dtf.document.iloc[0]\n","doc = spacy_tokenizer(normalize_unicode_text(grandson_document.text))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["grandson_tokens = [token for token in doc if token.text ==\"Grandson\"]\n","\n","grandson_tokens"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["seigneurs_tokens = [token for token in doc if token.text ==\"seigneurs\"]\n","[seigneurs_tokens[0].nbor(i) for i in range(-5,5)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nb_prev = 2\n","\n","grandson_tokens_3g = [[t.nbor(i) for i in range(-nb_prev,1)] for t in grandson_tokens]\n","\n","grandson_tokens_3g"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","Strategy rule-based annotations:\n","- tokenize list of articles\n","    - use doc text\n","    - normalize\n","- frequency table of N predecessor words\n","    -> identify the ones that are relevant statuswords\n","- extract all sequences of the form statuswords-X-X-toponym\n","- identify the relevant sequences representing an entity\n","- identify which entity each sequence corresponds to\n","    -> each statuswords refers to a list of possible entity type\n","    -> entity type + statusword\n","\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["polities_dtf.drop(\n","    [\"normalized_text\", \"len_normalized_text\", \"len_unnormalized_text\",\n","    \"len_diff_normalized_text\", \"len_diff_normalized_text\"],\n","    axis=1,\n","    inplace=True\n",")\n","if False:\n","\n","    def normalize_doc_text(d):\n","        \"\"\" /!\\\\ use with caution, see above\"\"\"\n","        d.text = normalize_unicode_text(d.text)\n","\n","    polities_dtf.document.apply(normalize_doc_text)\n","    \"\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sampled_articles_ids = set(sampled_articles_ids)\n","#sampled_polities_dtf = polities_dtf[polities_dtf.hds_article_id.apply(lambda id: id in sampled_articles_ids)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["polities_dtf.geoidentifier.unique()"]},{"cell_type":"markdown","metadata":{},"source":["#  Polity recognition\n","\n","## Identifying toponyms' tokens"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# take into account the fact that toponym might span multiple tokens\n","#articles_dtf[\"tokenized_toponym\"] = articles_dtf.toponym.apply(lambda t: set([tok.text for tok in spacy_tokenizer(normalize_unicode_text(t))]))\n","add_toponyms(polities_dtf, spacy_tokenizer)\n","toponym_tokens = polities_dtf[\"tokenized_toponym\"].explode().apply(lambda t: t.text)\n","#[t for t in utoponym_tokens if len(t)==4]\n","toponym_tokens_value_counts = toponym_tokens.value_counts()\n","toponym_tokens_value_counts[toponym_tokens_value_counts>1].shape\n","toponym_tokens_value_counts.shape\n","\n","\n","#with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n","toponym_tokens_value_counts"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["not_toponym_tokens = {\"'\",\n"," '-',\n"," '/',\n"," \"L'\",\n"," 'La',\n"," 'Lac',\n"," 'Le',\n"," 'Les',\n"," 'Nord',\n"," 'S',\n"," 'See',\n"," 'Sud',\n"," 'Sur',\n"," 'am',\n"," 'an',\n"," 'bei',\n"," 'ch',\n"," \"d'\",\n"," 'da',\n"," 'dans',\n"," 'de',\n"," 'der',\n"," 'des',\n"," 'di',\n"," 'du',\n"," 'et',\n"," 'im',\n"," 'in',\n"," 'l',\n"," 'la',\n"," 'le',\n"," 'les',\n"," 'près',\n"," 'sur',\n"," 'zum',\n"," \"vers\"\n","}\n","ambiguous_toponym_tokens={\n"," 'Au',\n"," 'Bois',\n"," 'Col',\n"," 'Dieu',\n"," 'Eaux',\n"," 'Ile',\n"," 'Part',\n"," 'Pays',\n"," 'Pont',\n"," 'Port',\n"," 'Rue',\n"," \"vaudois\",\n"," \"helvétique\",\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#articles_dtf[\"loose_normalized_tokenized_toponym\"] = [[s for s in texts if s not in not_toponym_tokens] for texts in articles_dtf[\"tokenized_toponym\"]]\n","#articles_dtf[\"strict_normalized_tokenized_toponym\"] = [[s for s in texts if s not in ambiguous_toponym_tokens] for texts in articles_dtf[\"loose_normalized_tokenized_toponym\"]]\n","\n","#normalized_toponym_tokens = set(articles_dtf[\"strict_normalized_tokenized_toponym\"].explode())\n","\n","normalized_toponym_tokens, trimmed_normalized_tokenized_toponyms_texts = normalize_toponyms(polities_dtf, not_toponym_tokens, ambiguous_toponym_tokens)\n","\n","polities_dtf.tokenized_toponym"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["additional_columns = [\n","    \"article\", \"document\", \"tokenized_toponym\",\n","    \"trimmed_normalized_tokenized_toponym\", \"loose_normalized_tokenized_toponym\", \"strict_normalized_tokenized_toponym\"]\n","articles_dtf = get_articles_dtf_from_polities_dtf(polities_dtf, additional_columns)\n","polities_dtf[\"tokenized_toponym_texts\"] = [[t.text for t in tokens] for tokens in polities_dtf.tokenized_toponym]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sampled_articles_dtf = articles_dtf[articles_dtf.hds_article_id.apply(lambda id: id in sampled_articles_ids)].copy()\n","\n","#sampled_articles_dtf[\"tokens\"] = sampled_articles_dtf.document.apply(lambda d: spacy_tokenizer(normalize_unicode_text(d.text)))\n","add_tokenized_text(sampled_articles_dtf, spacy_tokenizer)\n","sampled_articles_dtf.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# toponyms_pattern regex test\n","if False:\n","    trimmed_normalized_tokenized_toponyms_texts\n","\n","    toponyms_pattern = re.compile(\"(\"+(r\")\\W|\\W(\".join(trimmed_normalized_tokenized_toponyms_texts))+\")\")\n","    text = \" \"+sampled_articles_dtf.document.iloc[0].text+\" \"\n","\n","    match_list = [m for m in toponyms_pattern.finditer(text)] #, re.IGNORECASE)]\n","    (text, match_list)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# might simple string operation be faster than regex? who knows...\n","def find_indices(string, substring):\n","    \"\"\"returns all the start+end boundaries of the occurences of the substring inside the string\"\"\"\n","    indices = [(index,len(substring)) for index in range(len(string)) if string.startswith(substring, index)]\n","    return indices\n","\n","a_string = \"the quick brown fox jumps over the lazy dog. the quick brown fox jumps over the lazy dog\"\n","# Find all indices of 'the'\n","indices = [index for index in range(len(a_string)) if a_string.startswith('the', index)]\n","print(indices)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# as add_text_toponyms_spans() takes 10min to run on 100 articles,\n","# here is a simple cache system.\n","\n","def serialize_spacy_span(spacy_span):\n","    return (spacy_span.start, spacy_span.end)\n","def unserialize_spacy_span(spacy_doc, serialized_span):\n","    return spacy.tokens.span.Span(spacy_doc, serialized_span[0], serialized_span[1])\n","def serialize_spacy_spans_series(spans_series):\n","    \"\"\"serialize a series of spans list: i.e. a dtf column containing, for each row, a list of spans\"\"\"\n","    return spans_series.apply(lambda spans: [serialize_spacy_span(s) for s in spans])\n","def unserialize_spacy_spans_column(dtf, doc_column, serialized_spans_column):\n","    return [\n","        [unserialize_spacy_span(row[doc_column], s) for s in row[serialized_spans_column]]\n","        for i, row in dtf.iterrows()\n","    ]\n","    #return spacy_doc.char_span(serialized_span[0], serialized_span[1])\n","\n","\n","\n","def save_toponym_tokens_spans(dtf, picklepath):\n","    pickle_dtf = dtf.loc[:,[\"hds_article_id\", \"toponym_tokens_spans\"]].copy()\n","    pickle_dtf[\"toponym_tokens_spans\"] = serialize_spacy_spans_series(pickle_dtf[\"toponym_tokens_spans\"])\n","    pickle_dtf.to_pickle(picklepath)\n","\n","def restore_toponym_tokens_spans(dtf, picklepath):\n","    \"\"\"/!\\ doesn't change dtf inplace, must assign result\"\"\"\n","    pickle_dtf = pd.read_pickle(picklepath)\n","    dtf = dtf.merge(pickle_dtf, on=\"hds_article_id\")\n","    dtf[\"toponym_tokens_spans\"] = unserialize_spacy_spans_column(dtf, \"tokens\", \"toponym_tokens_spans\")\n","    return dtf\n","\n","def restore_or_compute_and_save_toponym_spans(dtf, picklepath, trimmed_normalized_tokenized_toponyms_texts):\n","    \"\"\"/!\\ doesn't change dtf inplace, must assign result\"\"\"\n","    if \"toponym_tokens_spans\" in dtf.columns:\n","        print('\"toponym_tokens_spans\" column already present in dataframe, no need to restore or compute.')\n","        return dtf\n","    if path.exists(picklepath):\n","        print(\"picklepath\",picklepath,\"exists, restoring...\")\n","        return restore_toponym_tokens_spans(dtf, picklepath)\n","    else:\n","        print(\"picklepath\",picklepath,\" not existing, computing toponym_tokens_spans...\")\n","        add_text_toponyms_spans(dtf, trimmed_normalized_tokenized_toponyms_texts)\n","        save_toponym_tokens_spans(dtf, picklepath)\n","        return dtf\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#add_text_toponyms_spans(sampled_articles_dtf, trimmed_normalized_tokenized_toponyms_texts)\n","\n","sampled_articles_dtf = restore_or_compute_and_save_toponym_spans(sampled_articles_dtf, s2_toponyms_spans_dtf_pickle, trimmed_normalized_tokenized_toponyms_texts)\n","sampled_articles_dtf.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def to_toponyms_dtf(dtf):\n","    return dtf.explode(\"toponym_tokens_spans\")\n","\n","toponyms_dtf = to_toponyms_dtf(sampled_articles_dtf)\n","toponyms_dtf.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Extracting toponym sequences"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nb_predecessors = 10\n","nb_successors = 3\n","add_toponym_tokens_sequence(toponyms_dtf, nb_predecessors, nb_successors)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# toponym_sequence_tokens: all tokens present in any toponym sequence\n","toponym_sequence_tokens = toponyms_dtf.toponym_tokens_sequence.apply(lambda span:\n","    [t.text for t in span]\n",").explode()\n","\n","toponym_sequence_tokens_value_counts = toponym_sequence_tokens.value_counts().to_frame()\n","toponym_sequence_tokens_value_counts.columns = ['toponym_sequence_tokens']\n","toponym_sequence_tokens_value_counts.to_csv(\"toponym_sequence_tokens_value_counts.csv\", sep=\"\\t\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["toponym_sequence_tokens_value_counts"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Differentiating single-toponym and statuswords-toponym sequences"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#pd.set_option('display.max_rows', None)\n","toponym_sequence_tokens_value_counts[toponym_sequence_tokens_value_counts.toponym_sequence_tokens==2]\n","s2_statuswords_json = path.join(s2_polities_to_extract_folder, \"statuswords.json\")\n","\n","with open(s2_statuswords_json) as f:\n","    statuswords_text = json.load(f)\n","\n","\n","ambiguous_statuswords_text = statuswords_text[\"ambiguous_statuswords\"]\n","statuswords_text = statuswords_text[\"statuswords\"]\n","statuswords_text\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["identify_statusword_toponym_sequences(toponyms_dtf, statuswords_text)\n","statusword_tokens_sequences_columns_filter =['hds_article_id', 'toponym', 'geoidentifier', 'article_title', 'polities_ids', 'nb_polities',\n","        'tokenized_toponym', \"trimmed_normalized_tokenized_toponym\", 'loose_normalized_tokenized_toponym',\n","        'strict_normalized_tokenized_toponym', \"toponym_tokens_spans\", 'toponym_tokens_sequence'\n","    ]\n","statusword_tokens_sequences_dtf = toponyms_dtf.loc[toponyms_dtf.is_statusword_toponym_sequence,statusword_tokens_sequences_columns_filter].copy()\n","only_toponyms_sequences_dtf = toponyms_dtf.loc[~toponyms_dtf.is_statusword_toponym_sequence,statusword_tokens_sequences_columns_filter].copy()\n","\n","print(\"total nb of identified toponyms:\", toponyms_dtf.shape[0])\n","print(\"nb of identified statusword+toponyms:\", statusword_tokens_sequences_dtf.shape[0])\n","print(\"nb of single toponyms:\",only_toponyms_sequences_dtf.shape[0])\n","print(\"check: nb of single+sw toponyms:\", statusword_tokens_sequences_dtf.shape[0]+only_toponyms_sequences_dtf.shape[0])\n","print(toponyms_dtf.toponym_tokens_sequence.iloc[0])\n","print(statusword_tokens_sequences_dtf.toponym_tokens_sequence.iloc[0][0].i)\n","print(only_toponyms_sequences_dtf.toponym_tokens_sequence.iloc[0][0].i)\n","toponyms_dtf.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Statusword-toponym combination\n","\n","### Analysing sequences structure\n","STATUS-XX-TOPONYM"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def analyse_statuswords_toponyms_sequences(dtf, statuswords_text, normalized_toponym_tokens):\n","    \"\"\"\n","    takes a dtf coming from identify_statuswords_toponyms_sequences()\n","    + adds \"sequence_analysis\" column to dtf (from analyse_statusword_tokens_sequence())\n","    + returns a new dtf sequences_analyses_dtf with one row per statusword+toponym combination (multiple rows possible for one toponym sequence)\"\"\"\n","    dtf[\"sequence_analysis\"] = [\n","        analyse_statusword_tokens_sequence(row, row.toponym_tokens_sequence, statuswords_text, normalized_toponym_tokens)\n","        for k, row in dtf.iterrows()\n","    ]\n","    sequences_analyses_dtf = dtf.explode(\"sequence_analysis\")\n","    non_analysable_sequences_indices = sequences_analyses_dtf.sequence_analysis.isna()\n","    non_analysable_sequences_dtf = sequences_analyses_dtf[non_analysable_sequences_indices].copy()\n","    sequences_analyses_dtf = sequences_analyses_dtf[~non_analysable_sequences_indices]\n","    sequences_analyses_dtf[\"statusword\"] = sequences_analyses_dtf.sequence_analysis.apply(lambda sa: sa[0])\n","    sequences_analyses_dtf[\"sequence_toponym\"] = sequences_analyses_dtf.sequence_analysis.apply(lambda sa: sa[1])\n","    sequences_analyses_dtf[\"sequence\"] = sequences_analyses_dtf.sequence_analysis.apply(lambda sa: sa[2])\n","    sequences_analyses_dtf[\"sequence_structure\"] = sequences_analyses_dtf.sequence_analysis.apply(lambda sa: sa[3])\n","    sequences_analyses_dtf[\"sequence_structure_str\"] = sequences_analyses_dtf[\"sequence_structure\"].apply(lambda ss: \"-\".join(ss))\n","    return sequences_analyses_dtf, non_analysable_sequences_dtf"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sequences_analyses_dtf, non_analysable_sequences_dtf = analyse_statuswords_toponyms_sequences(statusword_tokens_sequences_dtf, statuswords_text, normalized_toponym_tokens)\n","sequence_structures = sequences_analyses_dtf[\"sequence_structure_str\"].value_counts()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sequence_structures\n","sequence_structures.to_frame().to_csv(s2_sequence_structures_counts_csv, sep=\"\\t\")\n","sequence_structures[sequence_structures>3]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sequence_structure = \"STATUS-\\n-Dizain-du-TOPONYM\"\n","\n","sequence_structures_human_columns = ['toponym', 'article_title', 'polities_ids', \"statusword\", \"sequence\", \"sequence_structure\"]\n","\n","sequences_analyses_dtf.loc[sequences_analyses_dtf[\"sequence_structure_str\"]==sequence_structure,sequence_structures_human_columns]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Isolating valid statusword-toponym sequences structures"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["valid_sequence_structures = pd.read_csv(s2_sequence_structures_validation_csv, sep=\"\\t\")\n","valid_sequence_structures = set(valid_sequence_structures[valid_sequence_structures.validity==\"yes\"].structure)\n","valid_sequence_structures"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sequence_structures.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["valid_sequences_dtf, invalid_sequences_dtf = validate_statuswords_toponyms_sequences(sequences_analyses_dtf, valid_sequence_structures)\n","valid_sequences_dtf.shape"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Reintegration of invalid statusword-toponym sequences as single-toponyms\n","\n","Reintegrates both invalid_sequences_dtf and non_analysable_sequences_dtf\n","\n","algo:\n","- remove all rows from invalid_sequences_dtf if toponym_tokens_spans is inside:\n","    + valid_sequences_dtf.toponym_tokens_spans\n","    + single_toponyms_sequences_dtf.toponym_tokens_spans\n","    -> create set of tokens\n","- remove duplicate invalid_sequences_dtf.toponym_tokens_spans\n","- reunite it with only_toponyms_sequences_dtf\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","# checking how spacy token/span equality&hashing works\n","# -> as we would like it to!\n","if False:\n","    doc1 = spacy_tokenizer(\"j'aime la crème glacée\")\n","    doc2 = spacy_tokenizer(\"la crème suisse est vraiment glacée\")\n","    doc1set = set([t for t in doc1])\n","    doc2set = set([t for t in doc2])\n","    print(\"doc1:\", doc1)\n","    for i,t in enumerate(doc1):\n","        print(i,t)\n","    print(\"doc2:\", doc2)\n","    for i,t in enumerate(doc2):\n","        print(i,t)\n","    print(\"membership test:\")\n","    for i,t in enumerate(doc2):\n","        print(i,t)\n","        print(\"\\t\\tt in doc1\", t in doc1, \"\\tt in doc1set\", t in doc1set)\n","        print(\"\\t\\tt in doc2\", t in doc2, \"\\tt in doc2set\", t in doc2set)\n","        print(\"\\t\\t\", \"[t1==t for t1 in doc1]\")\n","        print(\"\\t\\t\", [t1==t for t1 in doc1])\n","        print(\"\\t\\t\", \"[t1==t for t1 in doc2]\")\n","        print(\"\\t\\t\", [t1==t for t1 in doc2])\n","\n","    print(\"\\nSpan tests:\")\n","    span1 = doc1[2:4]\n","    span2 = doc1[2:4]\n","    span3 = doc2[2:4]\n","    span4 = doc2[0:2]\n","    print(\"spans: 1\", span1, \"2: \", span2, \"3: \", span3, \"4: \", span4)\n","    print(\"1==1:\", span1==span1, \"1==2:\", span1==span2, \"1==3:\", span1==span3, \"1==4:\", span1==span4)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(list(non_analysable_sequences_dtf.shape))\n","print(list(invalid_sequences_dtf.shape))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["valid_sequence_toponyms = set([ t for t in valid_sequences_dtf.sequence_toponym])\n","sum([t in valid_sequence_toponyms for t in invalid_sequences_dtf.sequence_toponym])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["invalid_sequences_dtf.toponym_tokens_spans.duplicated().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"reintegrating invalid_sequences_dtf:\\n-----\")\n","single_toponyms_sequences_dtf = reintegrate_dtf(invalid_sequences_dtf, only_toponyms_sequences_dtf, [valid_sequences_dtf])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"reintegrating non_analysable_sequences_dtf:\\n-----\")\n","single_toponyms_sequences_dtf = reintegrate_dtf(non_analysable_sequences_dtf, single_toponyms_sequences_dtf, [valid_sequences_dtf])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Getting polities_dtf toponyms' tokens"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["polities_dtf[polities_dtf.typology==\"baillage\"].tail()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#polities_dtf[\"tokenized_toponym\"] = [spacy_tokenizer(t) for t in tqdm(polities_dtf.toponym, total=polities_dtf.shape[0], desc=\"Tokenizing polities' toponyms\")]\n","\n","polities_dtf[\"tokenized_toponym\"].apply(len).value_counts()\n","polities_dtf[polities_dtf[\"tokenized_toponym\"].apply(len)>1]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Polity linking"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open(s2_statusword_to_typology_json) as f:\n","    statusword_keys_dict = json.load(f)\n","\n","statusword_to_typology_dict = {\n","    statusword : t[1] \n","    for t in statusword_keys_dict\n","    for statusword in t[0]\n","}\n","\n","statusword_to_hdstag_dict = {\n","    statusword : t[2] \n","    for t in statusword_keys_dict\n","    for statusword in t[0]\n","}"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","## Linking single toponyms to their polity "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["default_hdstag_priorization = statusword_keys_dict[0][2]\n","default_hdstag_priorization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["toponyms_exact_match_dict = create_toponyms_exact_match_dict(polities_dtf, default_hdstag_priorization)\n","toponyms_exact_match_dict"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#single_toponyms_sequences_dtf.toponym_tokens_spans[single_toponyms_sequences_dtf.toponym_tokens_spans.apply(len)>1].head()#iloc[1].text\n","link_single_toponyms(single_toponyms_sequences_dtf, polities_dtf, toponyms_exact_match_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["single_toponyms_sequences_dtf.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["single_toponyms_sequences_dtf.possible_polities.apply(len).value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["single_toponyms_sequences_dtf[single_toponyms_sequences_dtf.possible_polities.apply(len)>4].head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","## Linking valid statuswords sequences to their polity "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def link_entity_by_typology(dtf_row, polities_dtf):\n","    possible_typologies = statusword_to_typology_dict.get(dtf_row.statusword.text.lower())\n","\n","    if possible_typologies is None:\n","        print(\"WARNING: statusword without corresponding typology: |\"+dtf_row.statusword.text.lower()+\"|\")\n","        return []\n","\n","    possible_polities = [\n","        polities_dtf.loc[(polities_dtf.typology==typology) & polities_dtf.toponym.apply(lambda t: dtf_row.sequence_toponym.text == t)]\n","        for typology in possible_typologies\n","    ]\n","    possible_polities = [dtf for dtf in possible_polities if dtf.shape[0]>0]\n","    return possible_polities\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["link_statuswords_toponyms_sequences(valid_sequences_dtf, polities_dtf, statusword_to_hdstag_dict)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["valid_sequences_dtf.possible_polities.iloc[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if False:\n","    valid_sequences_dtf[\"possible_polities_by_typology\"] = [\n","        link_entity_by_typology(row, polities_dtf)\n","        for i, row in valid_sequences_dtf.iterrows()\n","    ]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["valid_sequences_dtf[\"possible_polities\"].apply(lambda pp_dtf: pp_dtf.shape[0]).value_counts()\n","valid_sequences_dtf[\"possible_polities_min_rank\"].value_counts()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Exploring linking results\n","\n","### Statusword-toponym linking results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["linked_sequences_human_columns = [\"hds_article_id\", \"statusword\", \"sequence_toponym\", \"sequence\", \"linked_polity_id\", \"linked_hds_tag\", \"linked_toponym\"]\n","\n","valid_sequences_dtf.loc[:,linked_sequences_human_columns]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["linked_sequences_dtf = valid_sequences_dtf.loc[valid_sequences_dtf[\"possible_polities\"].apply(lambda pp: pp.shape[0]>0)].copy()\n","linked_sequences_dtf.loc[:,linked_sequences_human_columns]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["unlinked_sequences_human_columns = [\"hds_article_id\", \"statusword\", \"sequence_toponym\", \"sequence\"]\n","\n","unlinked_sequences_dtf = valid_sequences_dtf.loc[valid_sequences_dtf.linked_polity_id.apply(lambda lpi: lpi is None)].copy()\n","\n","print(\"Number of unlinked statusword-toponym sequences:\", unlinked_sequences_dtf.shape[0])\n","unlinked_sequences_dtf.loc[:,unlinked_sequences_human_columns]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["unlinked_sequences_dtf[\"sequence_text\"] = [\"\".join([t.text_with_ws for t in s]) for s in unlinked_sequences_dtf.sequence]\n","unlinked_sequences_dtf.sequence_text.value_counts().head(20)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["polities_dtf[polities_dtf.typology.apply(lambda t: t is None)].hds_tag.value_counts()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Statusword-toponyms + single toponyms linking results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["single_toponyms_sequences_dtf.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#valid_sequences_dtf.head()\n","valid_sequences_dtf[[len(t)>1 for t in valid_sequences_dtf.toponym_tokens_spans]][\n","    [\"toponym_tokens_spans\", \"sequence\", \"sequence_toponym\", \n","    \"linked_polity_id\", 'linked_hds_tag', 'linked_toponym']\n","].head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["statusword_toponyms_columns = list(valid_sequences_dtf.columns)\n","single_toponyms_columns = list(single_toponyms_sequences_dtf.columns)\n","\n","sw_columns = [c for c in statusword_toponyms_columns if c not in single_toponyms_columns]\n","otop_columns = [c for c in single_toponyms_columns if c not in statusword_toponyms_columns]\n","common_columns = [c for c in single_toponyms_columns if c in statusword_toponyms_columns]\n","\n","\n","print(\"Only statusword-toponym columns:\", sw_columns,\"\\n\")\n","print(\"Only single toponym columns:\", otop_columns,\"\\n\")\n","print(\"common columns:\", common_columns,\"\\n\")\n","\n","# -> columns to keep from valid_sequences_dtf are 'statusword', 'sequence'\n","statusword_columns_to_keep= ['statusword', 'sequence']\n","linked_toponyms_columns = [\n","    c for c in statusword_toponyms_columns\n","    if c in single_toponyms_columns or c in statusword_columns_to_keep\n","]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Putting together all the identified polities mentions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for c in statusword_columns_to_keep:\n","    single_toponyms_sequences_dtf[c] = None\n","\n","all_identified_polities = pd.concat([\n","    valid_sequences_dtf[linked_toponyms_columns],\n","    single_toponyms_sequences_dtf[linked_toponyms_columns]\n","])\n","print(\"Verification that we have the same nb of entries in toponyms_dtf and all_identified_polities:\")\n","print(\"- nb entries in toponyms_dtf: \",toponyms_dtf.shape[0])\n","print(\"- nb entries in all_identified_polities: \", all_identified_polities.shape[0])\n","print(\"- nb of duplicate entries in all_identified_polities: \", all_identified_polities.toponym_tokens_spans.duplicated().sum())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","all_identified_polities_spans = set(all_identified_polities.toponym_tokens_spans)\n","missing_toponyms = toponyms_dtf[toponyms_dtf.toponym_tokens_spans.apply(lambda tts:\n","    tts not in all_identified_polities_spans\n",")]\n","missing_toponyms.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["missing_toponyms_spans = set(missing_toponyms.toponym_tokens_spans)\n","\n","statusword_tokens_sequences_dtf[statusword_tokens_sequences_dtf.toponym_tokens_spans.apply(lambda tts:\n","    tts  in missing_toponyms_spans\n",")].shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["non_analysable_sequences_dtf.shape"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Annotating linked polities in documents"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def add_annotation_to_document_from_valid_sequences(document, valid_sequences_dtf_rows):\n","    new_annotations = [\n","        Annotation(\n","            row.toponym_tokens_spans[0].idx,\n","            row.toponym_tokens_spans[-1].idx+len(row.toponym_tokens_spans[-1]),\n","            extra_fields={\n","                \"type\": \"polity_id_LOC\",\n","                \"polity_id\": row.linked_polity_id\n","            }\n","        )\n","        for i, row in valid_sequences_dtf_rows.iterrows()\n","    ]\n","    document.annotations = document.annotations + new_annotations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i, row in sampled_articles_dtf.iterrows():\n","    add_annotation_to_document_from_valid_sequences(row.document, all_identified_polities[all_identified_polities.hds_article_id==row.hds_article_id])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# COMPLETING ANNOTATIONS OF MULTI-TOKEN TOPONYMS\n","sampled_articles_dtf.iloc[32,:].hds_article_id\n","\n","all_identified_polities[all_identified_polities.hds_article_id==\"001245\"]\n","dtf_row = all_identified_polities[all_identified_polities.hds_article_id==\"001245\"].iloc[1,:]\n","\n","all_identified_polities[all_identified_polities.hds_article_id==\"001245\"].iloc[0,:].possible_polities\n","\n","#hds_tag = valid_sequences_dtf.loc[valid_sequences_dtf.hds_article_id==\"001245\",[\"hds_tag\"]]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_values = [\n","    valid_sequences_dtf.shape[0],      statusword_tokens_sequences_dtf.shape[0]\n","]\n","truth_sequence = [\n","    valid_sequences_dtf.shape[0]==727, statusword_tokens_sequences_dtf.shape[0]==1456\n","]\n","\n","print(all(truth_sequence))\n","print(test_values)\n","print(truth_sequence)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test = linked_sequences_dtf.merge(polities_dtf.loc[:,[\"polity_id\", \"tokenized_toponym_texts\"]], left_on=\"linked_polity_id\", right_on=\"polity_id\", how=\"left\")\n","test.tokenized_toponym_texts.apply(len).value_counts()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_identified_polities.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["[a for a in sampled_articles_dtf.document.iloc[0].annotations]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"b0ba8461042d62ab06f66d67cc9ca318298fd283d556a98f37e80cf0f5e2bbb1"}}},"nbformat":4,"nbformat_minor":2}
