{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import unicodedata\n","\n","import sys\n","import re\n","from os import path\n","\n","sys.path.append(\"../../../src\")\n","sys.path.append(\"../../../scripts\")\n","from inception_fishing import Annotation\n","\n","from tqdm import tqdm\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from s2_prepare_articles import *\n","import spacy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["spacy_tokenizer = spacy.load(\"fr_core_news_sm\")"]},{"cell_type":"markdown","metadata":{},"source":["# Documents' text preparation\n","## Normalizing texts and investigating text issues"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Evaluating normalized text difference with original text:\n","\n","polities_dtf[\"normalized_text\"] = polities_dtf.document.apply(lambda d: unicodedata.normalize(\"NFKC\",d.text))\n","\n","polities_dtf[\"len_normalized_text\"] = polities_dtf[\"normalized_text\"].apply(len)\n","polities_dtf[\"len_unnormalized_text\"] = polities_dtf.document.apply(lambda d: len(d.text))\n","polities_dtf[\"len_diff_normalized_text\"] = polities_dtf[\"len_unnormalized_text\"] - polities_dtf[\"len_normalized_text\"]\n","polities_dtf[\"len_diff_normalized_text\"].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Character normalization investigation -> NFKC is the way to go :-)\n","\n","# Zoug had a very big 2529 char diff using NFKD\n","lendif2529NFKD = polities_dtf[polities_dtf[\"hds_article_id\"]==\"007373\"]\n","\n","def investigate_norm_len_diff(dtf, i=0):\n","    norm_text = dtf[\"normalized_text\"].iloc[i]\n","    unnorm_text = dtf.document.iloc[i].text\n","\n","    norm_chars = set(norm_text)\n","    unnorm_chars = set(unnorm_text)\n","\n","    unnorm_missing_chars = [ c for c in unnorm_chars if c not in norm_chars]\n","    norm_missing_chars = [ c for c in norm_chars if c not in unnorm_chars]\n","\n","    #return (norm_text, unnorm_text, norm_chars, unnorm_chars, norm_missing_chars, unnorm_missing_chars)\n","    return (norm_missing_chars, unnorm_missing_chars)\n","\n","investigate_norm_len_diff(lendif2529NFKD)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lendifNFKC = polities_dtf[polities_dtf[\"len_diff_normalized_text\"]!=0]\n","\n","\"\"\"Still some problem with some minor characters... we'll come to it when we need ta\"\"\"\n","polities_dtf.loc[polities_dtf.polity_id.apply(lambda i: i in [\"001256-c\", \"001321-c\", \"007384-ct\"]),:]\n","\n","\n","[investigate_norm_len_diff(lendifNFKC, i) for i in range(lendifNFKC.shape[0])]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def normalize_unicode_text(text):\n","    \"\"\"unicode normalization NFKD removes accents in characters -> NFKC is the way to go :-)\n","    \n","        /!\\\\ /!\\\\ /!\\\\ normalizing text yields different text lengths for 3 articles (out of 4000), use with caution /!\\\\/!\\\\/!\\\\\n","        but it is needed for proper tokenization/prediction/learning\"\"\"\n","\n","    return unicodedata.normalize(\"NFKC\",text)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["grandson_dtf = polities_dtf[polities_dtf.toponym==\"Grandson\"]\n","grandson_article = grandson_dtf.article.iloc[0]\n","grandson_document = grandson_dtf.document.iloc[0]\n","doc = spacy_tokenizer(normalize_unicode_text(grandson_document.text))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["grandson_tokens = [token for token in doc if token.text ==\"Grandson\"]\n","\n","grandson_tokens"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["seigneurs_tokens = [token for token in doc if token.text ==\"seigneurs\"]\n","[seigneurs_tokens[0].nbor(i) for i in range(-5,5)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nb_prev = 2\n","\n","grandson_tokens_3g = [[t.nbor(i) for i in range(-nb_prev,1)] for t in grandson_tokens]\n","\n","grandson_tokens_3g"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","Strategy rule-based annotations:\n","- tokenize list of articles\n","    - use doc text\n","    - normalize\n","- frequency table of N predecessor words\n","    -> identify the ones that are relevant statuswords\n","- extract all sequences of the form statuswords-X-X-toponym\n","- identify the relevant sequences representing an entity\n","- identify which entity each sequence corresponds to\n","    -> each statuswords refers to a list of possible entity type\n","    -> entity type + statusword\n","\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["polities_dtf.drop(\n","    [\"normalized_text\", \"len_normalized_text\", \"len_unnormalized_text\",\n","    \"len_diff_normalized_text\", \"len_diff_normalized_text\"],\n","    axis=1,\n","    inplace=True\n",")\n","if False:\n","\n","    def normalize_doc_text(d):\n","        \"\"\" /!\\\\ use with caution, see above\"\"\"\n","        d.text = normalize_unicode_text(d.text)\n","\n","    polities_dtf.document.apply(normalize_doc_text)\n","    \"\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sampled_articles_ids = set(sampled_articles_ids)\n","#sampled_polities_dtf = polities_dtf[polities_dtf.hds_article_id.apply(lambda id: id in sampled_articles_ids)]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["polities_dtf.geoidentifier.unique()"]},{"cell_type":"markdown","metadata":{},"source":["#  Polity recognition\n","\n","## Identifying toponyms' tokens"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def add_tokenized_text(dtf, tokenizer):\n","    \"\"\"\n","    takes a dtf with \"document\" column\n","    + adds the following columns:\n","        - tokens: spacy tokenization of text column\n","    \"\"\"\n","    dtf[\"tokens\"] = dtf.document.apply(lambda d: tokenizer(normalize_unicode_text(d.text)))\n","    return dtf\n","\n","def add_toponyms(dtf, tokenizer):\n","    \"\"\"\n","    takes a dtf with a \"toponym\" column\n","    + adds the following columns:\n","        - tokenized_toponym: spacy tokenization texts of toponym\n","    \"\"\"\n","    dtf[\"tokenized_toponym\"] = [tokenizer(t) for t in tqdm(dtf.toponym, total=dtf.shape[0], desc=\"Tokenizing toponyms\")]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# take into account the fact that toponym might span multiple tokens\n","#articles_dtf[\"tokenized_toponym\"] = articles_dtf.toponym.apply(lambda t: set([tok.text for tok in spacy_tokenizer(normalize_unicode_text(t))]))\n","add_toponyms(polities_dtf, spacy_tokenizer)\n","toponym_tokens = polities_dtf[\"tokenized_toponym\"].explode().apply(lambda t: t.text)\n","#[t for t in utoponym_tokens if len(t)==4]\n","toponym_tokens_value_counts = toponym_tokens.value_counts()\n","toponym_tokens_value_counts[toponym_tokens_value_counts>1].shape\n","toponym_tokens_value_counts.shape\n","\n","\n","#with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n","toponym_tokens_value_counts"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["not_toponym_tokens = {\"'\",\n"," '-',\n"," '/',\n"," \"L'\",\n"," 'La',\n"," 'Lac',\n"," 'Le',\n"," 'Les',\n"," 'Nord',\n"," 'S',\n"," 'See',\n"," 'Sud',\n"," 'Sur',\n"," 'am',\n"," 'an',\n"," 'bei',\n"," 'ch',\n"," \"d'\",\n"," 'da',\n"," 'dans',\n"," 'de',\n"," 'der',\n"," 'des',\n"," 'di',\n"," 'du',\n"," 'et',\n"," 'im',\n"," 'in',\n"," 'l',\n"," 'la',\n"," 'le',\n"," 'les',\n"," 'près',\n"," 'sur',\n"," 'zum',\n"," \"vers\"\n","}\n","ambiguous_toponym_tokens={\n"," 'Au',\n"," 'Bois',\n"," 'Col',\n"," 'Dieu',\n"," 'Eaux',\n"," 'Ile',\n"," 'Part',\n"," 'Pays',\n"," 'Pont',\n"," 'Port',\n"," 'Rue',\n"," \"vaudois\",\n"," \"helvétique\",\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def normalize_toponyms(dtf, not_toponym_tokens_texts, ambiguous_toponym_tokens_texts):\n","    \"\"\"\n","    normalize_toponyms(): takes a dtf coming from add_toponyms()\n","    + adds the following columns:\n","        - loose_normalized_tokenized_toponym: all toponym tokens that are generally toponym (exlcuding \"les\", \"la\", etc...)\n","        - strict_normalized_tokenized_toponym: same as loose_normalized_tokenized_toponym, excluding in addition ambiguous toponym tokens (\"eaux\", \"ile\", \"bois\", \"col\", etc...)\n","        - trimmed_normalized_tokenized_toponym: removing leading tokens that are not_toponym_tokens (mainly to avoid confusion with regard to toponyms such as \"Les Verrières\")\n","    + returns:\n","        - normalized_toponym_tokens: set of all strict_normalized_tokenized_toponym\n","    \"\"\"\n","    dtf[\"trimmed_normalized_tokenized_toponym\"] = [\n","        [s for s in tokens[:1] if s.text not in not_toponym_tokens_texts]+\n","        [t for t in tokens[1:]]\n","        for tokens in dtf[\"tokenized_toponym\"]\n","    ]   \n","    dtf[\"loose_normalized_tokenized_toponym\"] = [[s.text for s in tokens if s.text not in not_toponym_tokens_texts] for tokens in dtf[\"tokenized_toponym\"]]\n","    dtf[\"strict_normalized_tokenized_toponym\"] = [[st for st in texts if st not in ambiguous_toponym_tokens_texts] for texts in dtf[\"loose_normalized_tokenized_toponym\"]]\n","    normalized_toponym_tokens = set(dtf[\"strict_normalized_tokenized_toponym\"].explode())\n","    trimmed_normalized_tokenized_toponyms_texts = [t for t in dtf[\"trimmed_normalized_tokenized_toponym\"].apply(lambda tokens: \"\".join([t.text+t.whitespace_ for t in tokens])) if len(t)>0]\n","    return normalized_toponym_tokens, trimmed_normalized_tokenized_toponyms_texts"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#articles_dtf[\"loose_normalized_tokenized_toponym\"] = [[s for s in texts if s not in not_toponym_tokens] for texts in articles_dtf[\"tokenized_toponym\"]]\n","#articles_dtf[\"strict_normalized_tokenized_toponym\"] = [[s for s in texts if s not in ambiguous_toponym_tokens] for texts in articles_dtf[\"loose_normalized_tokenized_toponym\"]]\n","\n","#normalized_toponym_tokens = set(articles_dtf[\"strict_normalized_tokenized_toponym\"].explode())\n","\n","normalized_toponym_tokens, trimmed_normalized_tokenized_toponyms_texts = normalize_toponyms(polities_dtf, not_toponym_tokens, ambiguous_toponym_tokens)\n","\n","polities_dtf.tokenized_toponym"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["additional_columns = [\n","    \"article\", \"document\", \"tokenized_toponym\",\n","    \"trimmed_normalized_tokenized_toponym\", \"loose_normalized_tokenized_toponym\", \"strict_normalized_tokenized_toponym\"]\n","articles_dtf = get_articles_dtf_from_polities_dtf(polities_dtf, additional_columns)\n","polities_dtf[\"tokenized_toponym_texts\"] = [[t.text for t in tokens] for tokens in polities_dtf.tokenized_toponym]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sampled_articles_dtf = articles_dtf[articles_dtf.hds_article_id.apply(lambda id: id in sampled_articles_ids)].copy()\n","\n","#sampled_articles_dtf[\"tokens\"] = sampled_articles_dtf.document.apply(lambda d: spacy_tokenizer(normalize_unicode_text(d.text)))\n","add_tokenized_text(sampled_articles_dtf, spacy_tokenizer)\n","sampled_articles_dtf.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def is_token_toponym(token, dtf_row, normalized_toponym_tokens):\n","    \"\"\"Checks that a given token is a toponym (either corresponding to any strict toponym, or a loose toponym from the particular article toponym\n","    \"\"\"\n","    return (\n","        token.text in normalized_toponym_tokens\n","        or token.text in dtf_row.loose_normalized_tokenized_toponym\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# toponyms_pattern regex test\n","if False:\n","    trimmed_normalized_tokenized_toponyms_texts\n","\n","    toponyms_pattern = re.compile(\"(\"+(r\")\\W|\\W(\".join(trimmed_normalized_tokenized_toponyms_texts))+\")\")\n","    text = \" \"+sampled_articles_dtf.document.iloc[0].text+\" \"\n","\n","    match_list = [m for m in toponyms_pattern.finditer(text)] #, re.IGNORECASE)]\n","    (text, match_list)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# might simple string operation be faster than regex? who knows...\n","def find_indices(string, substring):\n","    \"\"\"returns all the start+end boundaries of the occurences of the substring inside the string\"\"\"\n","    indices = [(index,len(substring)) for index in range(len(string)) if string.startswith(substring, index)]\n","    return indices\n","\n","a_string = \"the quick brown fox jumps over the lazy dog. the quick brown fox jumps over the lazy dog\"\n","# Find all indices of 'the'\n","indices = [index for index in range(len(a_string)) if a_string.startswith('the', index)]\n","print(indices)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def add_text_toponyms_spans(dtf, trimmed_normalized_tokenized_toponyms_texts):\n","    \"\"\"add_text_toponyms_spans():  takes a dtf coming from normalize_toponyms()\n","    + adds the following columns: \n","        - toponym_tokens_spans: list of spacy Spans, each Span containing a toponym, ensures no overlaps\n","\n","    algorithm:\n","    The pattern requires whitespace before&after toponym,\n","    hence we add whitespace at beginning and end of text to ensure detection at those places\n","    and correct match indices afterwards\n","    \n","    example:\n","    real text:\n","    \"MEIERKAPPEL, BLABLA\"\t    -> real bounds 0, 11\n","    with added space:\n","    \" MEIERKAPPEL, BLABLA\" -> displaced bounds 1, 12\n","    detected:\n","    \" MEIERKAPPEL,\"         -> detected bounds 0, 13\n","\n","    real_start = m.start()+1-1, +1 because \\W in pattern, -1 because \" \" added in front of text\n","    real_end = m.end()-2 because \\W at start and end of pattern\n","    \n","    \"\"\"\n","\n","    toponyms_pattern = re.compile(\"(\"+(r\")\\W|\\W(\".join(trimmed_normalized_tokenized_toponyms_texts))+\")\")\n","\n","    # multi-tokens toponyms\n","    dtf[\"toponym_tokens_spans\"]=[\n","        [\n","            row.tokens.char_span(m.start(), m.end()-2, alignment_mode=\"contract\")\n","            for m in toponyms_pattern.finditer(\" \"+row.document.text+\" \") #, re.IGNORECASE):\n","        ]\n","        for i, row in tqdm(dtf.iterrows(), total = dtf.shape[0], desc =\"Adding token spans\")\n","    ]\n","    dtf[\"toponym_tokens_spans\"] = dtf[\"toponym_tokens_spans\"].apply(lambda spans: [s for s in spans if s is not None])\n","    dtf[\"toponym_tokens_indices\"] = dtf[\"toponym_tokens_spans\"].apply(lambda spans: set([t.i for span in spans for t in span]))\n","\n","    # single token toponyms that are in the row.loose_normalized_tokenized_toponym (think \"Au\", \"le Pont\", \"See\", etc...)\n","    dtf[\"toponym_tokens_spans\"] = [\n","        row.toponym_tokens_spans+\n","        [\n","            row.tokens[token.i:(token.i+1)] for token in row.tokens\n","            if \n","                token.i not in row.toponym_tokens_indices and # ensure we don't have twice the same toponyms\n","                token.text in row.loose_normalized_tokenized_toponym\n","        ]\n","            for i, row in dtf.iterrows()\n","    ]\n","    del dtf[\"toponym_tokens_indices\"]\n","        \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# as add_text_toponyms_spans() takes 10min to run on 100 articles,\n","# here is a simple cache system.\n","\n","def serialize_spacy_span(spacy_span):\n","    return (spacy_span.start, spacy_span.end)\n","def unserialize_spacy_span(spacy_doc, serialized_span):\n","    return spacy.tokens.span.Span(spacy_doc, serialized_span[0], serialized_span[1])\n","def serialize_spacy_spans_series(spans_series):\n","    \"\"\"serialize a series of spans list: i.e. a dtf column containing, for each row, a list of spans\"\"\"\n","    return spans_series.apply(lambda spans: [serialize_spacy_span(s) for s in spans])\n","def unserialize_spacy_spans_column(dtf, doc_column, serialized_spans_column):\n","    return [\n","        [unserialize_spacy_span(row[doc_column], s) for s in row[serialized_spans_column]]\n","        for i, row in dtf.iterrows()\n","    ]\n","    #return spacy_doc.char_span(serialized_span[0], serialized_span[1])\n","\n","\n","\n","def save_toponym_tokens_spans(dtf, picklepath):\n","    pickle_dtf = dtf.loc[:,[\"hds_article_id\", \"toponym_tokens_spans\"]].copy()\n","    pickle_dtf[\"toponym_tokens_spans\"] = serialize_spacy_spans_series(pickle_dtf[\"toponym_tokens_spans\"])\n","    pickle_dtf.to_pickle(picklepath)\n","\n","def restore_toponym_tokens_spans(dtf, picklepath):\n","    \"\"\"/!\\ doesn't change dtf inplace, must assign result\"\"\"\n","    pickle_dtf = pd.read_pickle(picklepath)\n","    dtf = dtf.merge(pickle_dtf, on=\"hds_article_id\")\n","    dtf[\"toponym_tokens_spans\"] = unserialize_spacy_spans_column(dtf, \"tokens\", \"toponym_tokens_spans\")\n","    return dtf\n","\n","def restore_or_compute_and_save_toponym_spans(dtf, picklepath, trimmed_normalized_tokenized_toponyms_texts):\n","    \"\"\"/!\\ doesn't change dtf inplace, must assign result\"\"\"\n","    if \"toponym_tokens_spans\" in dtf.columns:\n","        print('\"toponym_tokens_spans\" column already present in dataframe, no need to restore or compute.')\n","        return dtf\n","    if path.exists(picklepath):\n","        print(\"picklepath\",picklepath,\"exists, restoring...\")\n","        return restore_toponym_tokens_spans(dtf, picklepath)\n","    else:\n","        print(\"picklepath\",picklepath,\" not existing, computing toponym_tokens_spans...\")\n","        add_text_toponyms_spans(dtf, trimmed_normalized_tokenized_toponyms_texts)\n","        save_toponym_tokens_spans(dtf, picklepath)\n","        return dtf\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#add_text_toponyms_spans(sampled_articles_dtf, trimmed_normalized_tokenized_toponyms_texts)\n","\n","sampled_articles_dtf = restore_or_compute_and_save_toponym_spans(sampled_articles_dtf, s2_toponyms_spans_dtf_pickle, trimmed_normalized_tokenized_toponyms_texts)\n","sampled_articles_dtf.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def to_toponyms_dtf(dtf):\n","    return dtf.explode(\"toponym_tokens_spans\")\n","\n","toponyms_dtf = to_toponyms_dtf(sampled_articles_dtf)\n","toponyms_dtf.head()"]},{"cell_type":"markdown","metadata":{},"source":["## Extracting toponym sequences"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def add_toponym_tokens_sequence(dtf, nb_predecessors = 10, nb_successors = 3):\n","    \"\"\"add_toponym_tokens_sequence(): takes nb_predecessors, nb_successors and a dtf coming from add_text_toponyms_spans() and add_tokenized_text()\n","    + adds the following columns:\n","        - toponym_tokens_sequence: for each toponym_token_span, a sequence according from nb_pred to nb_succ (indexed on first token from each span)\"\"\"\n","\n","    dtf[\"toponym_tokens_sequence\"] = dtf.toponym_tokens_spans.apply(lambda span: [\n","        span[0].nbor(i)\n","        for i in range(\n","            -min(nb_predecessors,span[0].i),\n","            min(nb_successors, len(span[0].doc)-span[0].i)\n","        )\n","    ])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nb_predecessors = 10\n","nb_successors = 3\n","add_toponym_tokens_sequence(toponyms_dtf, nb_predecessors, nb_successors)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# toponym_sequence_tokens: all tokens present in any toponym sequence\n","toponym_sequence_tokens = toponyms_dtf.toponym_tokens_sequence.apply(lambda span:\n","    [t.text for t in span]\n",").explode()\n","\n","toponym_sequence_tokens_value_counts = toponym_sequence_tokens.value_counts().to_frame()\n","toponym_sequence_tokens_value_counts.columns = ['toponym_sequence_tokens']\n","toponym_sequence_tokens_value_counts.to_csv(\"toponym_sequence_tokens_value_counts.csv\", sep=\"\\t\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["toponym_sequence_tokens_value_counts"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Statusword-toponym combination\n","### Identifying statuswords"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#pd.set_option('display.max_rows', None)\n","toponym_sequence_tokens_value_counts[toponym_sequence_tokens_value_counts.toponym_sequence_tokens==2]\n","s2_statuswords_json = path.join(s2_polities_to_extract_folder, \"statuswords.json\")\n","\n","with open(s2_statuswords_json) as f:\n","    statusword_token_text = json.load(f)\n","\n","\n","ambiguous_statusword_token_text = statusword_token_text[\"ambiguous_statuswords\"]\n","statusword_token_text = statusword_token_text[\"statuswords\"]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def identify_statusword_toponym_sequences(dtf, statusword_token_text):\n","    \"\"\"\n","    takes a dtf coming from add_toponym_tokens_sequence()\n","    + adds a column \"toponym_tokens_sequence\" to dtf containing all the toponym_tokens_sequence also containing a statusword\n","    + returns a new dtf statusword_tokens_sequences_dtf with one row per sequence containing at least 1 statusword and 1 toponym\n","    \"\"\"\n","    dtf[\"is_statusword_toponym_sequence\"] = [\n","            any(token.text.lower() in statusword_token_text for token in seq)\n","            for seq in dtf.toponym_tokens_sequence\n","    ]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["identify_statusword_toponym_sequences(toponyms_dtf, statusword_token_text)\n","statusword_tokens_sequences_columns_filter =['hds_article_id', 'toponym', 'geoidentifier', 'article_title', 'polities_ids', 'nb_polities',\n","        'tokenized_toponym', \"trimmed_normalized_tokenized_toponym\", 'loose_normalized_tokenized_toponym',\n","        'strict_normalized_tokenized_toponym', \"toponym_tokens_spans\", 'toponym_tokens_sequence'\n","    ]\n","statusword_tokens_sequences_dtf = toponyms_dtf.loc[toponyms_dtf.is_statusword_toponym_sequence,statusword_tokens_sequences_columns_filter].copy()\n","only_toponyms_sequences_dtf = toponyms_dtf.loc[~toponyms_dtf.is_statusword_toponym_sequence,statusword_tokens_sequences_columns_filter].copy()\n","\n","(\n","    toponyms_dtf.shape,\n","    statusword_tokens_sequences_dtf.shape[0]+only_toponyms_sequences_dtf.shape[0],\n","    statusword_tokens_sequences_dtf.shape, only_toponyms_sequences_dtf.shape\n",")\n","print(toponyms_dtf.toponym_tokens_sequence.iloc[0])\n","print(statusword_tokens_sequences_dtf.toponym_tokens_sequence.iloc[0][0].i)\n","print(only_toponyms_sequences_dtf.toponym_tokens_sequence.iloc[0][0].i)\n","toponyms_dtf.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Analysing sequences structure\n","STATUS-XX-TOPONYM"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def analyse_statusword_tokens_sequence_single(dtf_row, token_sequence, statusword_index, toponym_index):\n","    \"\"\"Analyses a single statusword-toponym combination\n","\n","    returns a sequence whose first term is the sequence's statusword, and the last word is the sequence's toponym\n","    \"\"\"\n","    sequence = token_sequence[statusword_index:(toponym_index+1)]\n","    sequence_structure = [\n","        \"STATUS\" if token.text.lower() in statusword_token_text else(\n","        \"TOPONYM\" if is_token_toponym(token, dtf_row, normalized_toponym_tokens)\n","        else token.text\n","        )\n","        for token in sequence\n","    ]\n","    statusword = token_sequence[statusword_index]\n","    toponym = token_sequence[toponym_index]\n","    return (statusword, toponym, sequence, sequence_structure)\n","\n","def analyse_statusword_tokens_sequence(dtf_row, token_sequence):\n","    \"\"\"Returns all the possible statusword-toponym combination analyses for a given token sequence\n","    \"\"\"\n","    statusword_indices = [i for i,tok in enumerate(token_sequence) if tok.text.lower() in statusword_token_text]\n","    #toponym_indices = [i for i,tok in enumerate(token_sequence) if tok.text in normalized_toponym_tokens or tok.text in dtf_row.loose_normalized_tokenized_toponym]\n","    toponym_indices = [i for i, t in enumerate(token_sequence) if t.i == dtf_row.toponym_tokens_spans[0].i]\n","    #toponym_indices = [len(token_sequence)-nb_successors] # the toponym is always at the same spot in the sequence\n","    sequences_analyses = [\n","        analyse_statusword_tokens_sequence_single(dtf_row, token_sequence, i, j)\n","        for i in statusword_indices for j in toponym_indices if i<j\n","    ]\n","    return sequences_analyses"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def analyse_statuswords_toponyms_sequences(dtf):\n","    \"\"\"\n","    takes a dtf coming from identify_statuswords_toponyms_sequences()\n","    + adds \"sequence_analysis\" column to dtf (from analyse_statusword_tokens_sequence())\n","    + returns a new dtf sequences_analyses_dtf with one row per statusword+toponym combination (multiple rows possible for one toponym sequence)\"\"\"\n","    dtf[\"sequence_analysis\"] = [\n","        analyse_statusword_tokens_sequence(row, row.toponym_tokens_sequence)\n","        for k, row in dtf.iterrows()\n","    ]\n","    sequences_analyses_dtf = dtf.explode(\"sequence_analysis\")\n","    sequences_analyses_dtf = sequences_analyses_dtf[~sequences_analyses_dtf.sequence_analysis.isna()]\n","    sequences_analyses_dtf[\"statusword\"] = sequences_analyses_dtf.sequence_analysis.apply(lambda sa: sa[0])\n","    sequences_analyses_dtf[\"sequence_toponym\"] = sequences_analyses_dtf.sequence_analysis.apply(lambda sa: sa[1])\n","    sequences_analyses_dtf[\"sequence\"] = sequences_analyses_dtf.sequence_analysis.apply(lambda sa: sa[2])\n","    sequences_analyses_dtf[\"sequence_structure\"] = sequences_analyses_dtf.sequence_analysis.apply(lambda sa: sa[3])\n","    sequences_analyses_dtf[\"sequence_structure_str\"] = sequences_analyses_dtf[\"sequence_structure\"].apply(lambda ss: \"-\".join(ss))\n","    return sequences_analyses_dtf"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sequences_analyses_dtf = analyse_statuswords_toponyms_sequences(statusword_tokens_sequences_dtf)\n","sequence_structures = sequences_analyses_dtf[\"sequence_structure_str\"].value_counts()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sequence_structures\n","sequence_structures.to_frame().to_csv(s2_sequence_structures_counts_csv, sep=\"\\t\")\n","sequence_structures[sequence_structures>3]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sequence_structure = \"STATUS-\\n-Dizain-du-TOPONYM\"\n","\n","sequence_structures_human_columns = ['toponym', 'article_title', 'polities_ids', \"statusword\", \"sequence\", \"sequence_structure\"]\n","\n","sequences_analyses_dtf.loc[sequences_analyses_dtf[\"sequence_structure_str\"]==sequence_structure,sequence_structures_human_columns]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Isolating valid statusword-toponym sequences structures"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["valid_sequence_structures = pd.read_csv(s2_sequence_structures_validation_csv, sep=\"\\t\")\n","valid_sequence_structures = set(valid_sequence_structures[valid_sequence_structures.validity==\"yes\"].structure)\n","valid_sequence_structures"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sequence_structures.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def validate_statuswords_toponyms_sequences(dtf, valid_sequence_structures):\n","    \"\"\"\n","    takes valid_sequence_structures set of strings and a dtf coming from explode_statuswords_toponyms_sequences()\n","    + returns a new dtf valid_sequences_dtf containing the valid statuswords_toponyms_sequences\n","    \"\"\"\n","    valid_sequences_dtf = dtf[dtf.sequence_structure_str.apply(lambda struct: struct in valid_sequence_structures)].copy()\n","    return valid_sequences_dtf"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["valid_sequences_dtf = validate_statuswords_toponyms_sequences(sequences_analyses_dtf, valid_sequence_structures)\n","valid_sequences_dtf.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open(s2_statusword_to_typology_json) as f:\n","    statusword_keys_dict = json.load(f)\n","\n","statusword_to_typology_dict = {\n","    statusword : t[1] \n","    for t in statusword_keys_dict\n","    for statusword in t[0]\n","}\n","\n","statusword_to_hdstag_dict = {\n","    statusword : t[2] \n","    for t in statusword_keys_dict\n","    for statusword in t[0]\n","}"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Getting polities_dtf toponyms' tokens"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["polities_dtf[polities_dtf.typology==\"baillage\"].tail()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#polities_dtf[\"tokenized_toponym\"] = [spacy_tokenizer(t) for t in tqdm(polities_dtf.toponym, total=polities_dtf.shape[0], desc=\"Tokenizing polities' toponyms\")]\n","\n","polities_dtf[\"tokenized_toponym\"].apply(len).value_counts()\n","polities_dtf[polities_dtf[\"tokenized_toponym\"].apply(len)>1]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Polity linking"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open(s2_statusword_to_typology_json) as f:\n","    statusword_keys_dict = json.load(f)\n","\n","statusword_to_typology_dict = {\n","    statusword : t[1] \n","    for t in statusword_keys_dict\n","    for statusword in t[0]\n","}\n","\n","statusword_to_hdstag_dict = {\n","    statusword : t[2] \n","    for t in statusword_keys_dict\n","    for statusword in t[0]\n","}"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","## Linking single toponyms to their polity "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["default_hdstag_priorization = statusword_keys_dict[0][2]\n","default_hdstag_priorization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","def create_toponyms_exact_match_dict(polities_dtf, default_hdstag_priorization):\n","    default_hdstag_priorization_dict = {t:i for i,t in enumerate(default_hdstag_priorization)}\n","    toponyms_exact_match_dict = {}\n","    for i, row in polities_dtf.iterrows():\n","        toponym_key = \"\".join([t.text+t.whitespace_ for t in row.trimmed_normalized_tokenized_toponym])\n","        toponym_possible_polities = toponyms_exact_match_dict.get(toponym_key)\n","        if toponym_possible_polities is None:\n","            toponyms_exact_match_dict[toponym_key] = [None]*len(default_hdstag_priorization_dict)\n","            toponym_possible_polities = toponyms_exact_match_dict[toponym_key]\n","        hdstag_priorization = default_hdstag_priorization_dict.get(row.hds_tag)\n","        if hdstag_priorization is not None:\n","            # if it has a priorization: put it in its proper priorization position\n","            toponym_possible_polities[hdstag_priorization] = row.polity_id\n","        else: \n","            # else append it at the end\n","            toponym_possible_polities.append(row.polity_id)\n","\n","    for k,v in toponyms_exact_match_dict.items():\n","        # remove None entries from the priorization:\n","        toponyms_exact_match_dict[k] = [pid for pid in v if pid is not None]\n","\n","    #pd.Series([len(v) for v in toponyms_exact_match_dict.values()]).value_counts()\n","    return toponyms_exact_match_dict"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["toponyms_exact_match_dict = create_toponyms_exact_match_dict(polities_dtf, default_hdstag_priorization)\n","toponyms_exact_match_dict"]},{"cell_type":"code","execution_count":92,"metadata":{},"outputs":[],"source":["def link_toponym_by_exact_match(toponym, toponyms_exact_match_dict):\n","    \"\"\"Links a single toponym to its possible polities by toponym exact match\n","    \"\"\"\n","    possible_polities= toponyms_exact_match_dict.get(toponym)\n","    return possible_polities if possible_polities is not None else []\n","\n","def link_single_toponyms(dtf, polities_dtf, toponyms_exact_match_dict):\n","    \"\"\"\n","    link_single_toponyms(): takes a dtf coming from identify_statusword_toponym_sequences()\n","    + links single toponyms to the polity ids found in the result from create_toponyms_exact_match_dict()\n","    + adds columns\n","        - possible_polities\n","        - linked_polity_id, linked_hds_tag, linked_toponym\n","    \"\"\"\n","    dtf[\"possible_polities\"] = dtf.toponym_tokens_spans.apply(lambda span:\n","        link_toponym_by_exact_match(span.text, toponyms_exact_match_dict)\n","    )\n","    dtf[\"linked_polity_id\"] = [(pp[0] if len(pp)>0 else None) for pp in dtf[\"possible_polities\"]]\n","    dtf[\"linked_hds_tag\"] = [(polities_dtf.hds_tag[polities_dtf.polity_id==lpi].iloc[0] if lpi is not None else None) for lpi in dtf[\"linked_polity_id\"]]\n","    dtf[\"linked_toponym\"] = [(polities_dtf.toponym[polities_dtf.polity_id==lpi].iloc[0] if lpi is not None else None) for lpi in dtf[\"linked_polity_id\"]]\n","\n"]},{"cell_type":"code","execution_count":94,"metadata":{},"outputs":[],"source":["#only_toponyms_sequences_dtf.toponym_tokens_spans[only_toponyms_sequences_dtf.toponym_tokens_spans.apply(len)>1].head()#iloc[1].text\n","link_single_toponyms(only_toponyms_sequences_dtf, polities_dtf, toponyms_exact_match_dict)"]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>hds_article_id</th>\n","      <th>toponym</th>\n","      <th>geoidentifier</th>\n","      <th>article_title</th>\n","      <th>polities_ids</th>\n","      <th>nb_polities</th>\n","      <th>tokenized_toponym</th>\n","      <th>trimmed_normalized_tokenized_toponym</th>\n","      <th>loose_normalized_tokenized_toponym</th>\n","      <th>strict_normalized_tokenized_toponym</th>\n","      <th>toponym_tokens_spans</th>\n","      <th>toponym_tokens_sequence</th>\n","      <th>possible_polities</th>\n","      <th>linked_polity_id</th>\n","      <th>linked_hds_tag</th>\n","      <th>linked_toponym</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>000627</td>\n","      <td>Meierskappel</td>\n","      <td>None</td>\n","      <td>Meierskappel</td>\n","      <td>[000627-c]</td>\n","      <td>1</td>\n","      <td>(Meierskappel)</td>\n","      <td>[Meierskappel]</td>\n","      <td>[Meierskappel]</td>\n","      <td>[Meierskappel]</td>\n","      <td>(Zoug)</td>\n","      <td>[s', étendant, du, flanc, sud-est, du, Rooterb...</td>\n","      <td>[000797-c, 007373-ct]</td>\n","      <td>000797-c</td>\n","      <td>Entités politiques / Commune</td>\n","      <td>Zoug</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>000627</td>\n","      <td>Meierskappel</td>\n","      <td>None</td>\n","      <td>Meierskappel</td>\n","      <td>[000627-c]</td>\n","      <td>1</td>\n","      <td>(Meierskappel)</td>\n","      <td>[Meierskappel]</td>\n","      <td>[Meierskappel]</td>\n","      <td>[Meierskappel]</td>\n","      <td>(Meierskappel)</td>\n","      <td>[selon, copie, du, XIVe, s., ), ;, le, nom, de...</td>\n","      <td>[000627-c]</td>\n","      <td>000627-c</td>\n","      <td>Entités politiques / Commune</td>\n","      <td>Meierskappel</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>000627</td>\n","      <td>Meierskappel</td>\n","      <td>None</td>\n","      <td>Meierskappel</td>\n","      <td>[000627-c]</td>\n","      <td>1</td>\n","      <td>(Meierskappel)</td>\n","      <td>[Meierskappel]</td>\n","      <td>[Meierskappel]</td>\n","      <td>[Meierskappel]</td>\n","      <td>(Zoug)</td>\n","      <td>[basse, justice, et, droits, de, pêche, dans, ...</td>\n","      <td>[000797-c, 007373-ct]</td>\n","      <td>000797-c</td>\n","      <td>Entités politiques / Commune</td>\n","      <td>Zoug</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>000627</td>\n","      <td>Meierskappel</td>\n","      <td>None</td>\n","      <td>Meierskappel</td>\n","      <td>[000627-c]</td>\n","      <td>1</td>\n","      <td>(Meierskappel)</td>\n","      <td>[Meierskappel]</td>\n","      <td>[Meierskappel]</td>\n","      <td>[Meierskappel]</td>\n","      <td>(Habsbourg)</td>\n","      <td>[Böschenrot, (, attestée, en, 1173, et, 1346/1...</td>\n","      <td>[007503-b]</td>\n","      <td>007503-b</td>\n","      <td>Entités politiques / Bailliage, châtellenie</td>\n","      <td>Habsbourg</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>000627</td>\n","      <td>Meierskappel</td>\n","      <td>None</td>\n","      <td>Meierskappel</td>\n","      <td>[000627-c]</td>\n","      <td>1</td>\n","      <td>(Meierskappel)</td>\n","      <td>[Meierskappel]</td>\n","      <td>[Meierskappel]</td>\n","      <td>[Meierskappel]</td>\n","      <td>(Lucerne)</td>\n","      <td>[au, Fraumünster, la, presqu', île, du, Chieme...</td>\n","      <td>[000624-c, 007382-ct, 007382-ct, 011152-b, 011...</td>\n","      <td>000624-c</td>\n","      <td>Entités politiques / Commune</td>\n","      <td>Lucerne</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  hds_article_id       toponym geoidentifier article_title polities_ids  \\\n","0         000627  Meierskappel          None  Meierskappel   [000627-c]   \n","0         000627  Meierskappel          None  Meierskappel   [000627-c]   \n","0         000627  Meierskappel          None  Meierskappel   [000627-c]   \n","0         000627  Meierskappel          None  Meierskappel   [000627-c]   \n","0         000627  Meierskappel          None  Meierskappel   [000627-c]   \n","\n","   nb_polities tokenized_toponym trimmed_normalized_tokenized_toponym  \\\n","0            1    (Meierskappel)                       [Meierskappel]   \n","0            1    (Meierskappel)                       [Meierskappel]   \n","0            1    (Meierskappel)                       [Meierskappel]   \n","0            1    (Meierskappel)                       [Meierskappel]   \n","0            1    (Meierskappel)                       [Meierskappel]   \n","\n","  loose_normalized_tokenized_toponym strict_normalized_tokenized_toponym  \\\n","0                     [Meierskappel]                      [Meierskappel]   \n","0                     [Meierskappel]                      [Meierskappel]   \n","0                     [Meierskappel]                      [Meierskappel]   \n","0                     [Meierskappel]                      [Meierskappel]   \n","0                     [Meierskappel]                      [Meierskappel]   \n","\n","  toponym_tokens_spans                            toponym_tokens_sequence  \\\n","0               (Zoug)  [s', étendant, du, flanc, sud-est, du, Rooterb...   \n","0       (Meierskappel)  [selon, copie, du, XIVe, s., ), ;, le, nom, de...   \n","0               (Zoug)  [basse, justice, et, droits, de, pêche, dans, ...   \n","0          (Habsbourg)  [Böschenrot, (, attestée, en, 1173, et, 1346/1...   \n","0            (Lucerne)  [au, Fraumünster, la, presqu', île, du, Chieme...   \n","\n","                                   possible_polities linked_polity_id  \\\n","0                              [000797-c, 007373-ct]         000797-c   \n","0                                         [000627-c]         000627-c   \n","0                              [000797-c, 007373-ct]         000797-c   \n","0                                         [007503-b]         007503-b   \n","0  [000624-c, 007382-ct, 007382-ct, 011152-b, 011...         000624-c   \n","\n","                                linked_hds_tag linked_toponym  \n","0                 Entités politiques / Commune           Zoug  \n","0                 Entités politiques / Commune   Meierskappel  \n","0                 Entités politiques / Commune           Zoug  \n","0  Entités politiques / Bailliage, châtellenie      Habsbourg  \n","0                 Entités politiques / Commune        Lucerne  "]},"execution_count":95,"metadata":{},"output_type":"execute_result"}],"source":["only_toponyms_sequences_dtf.head()"]},{"cell_type":"code","execution_count":96,"metadata":{},"outputs":[{"data":{"text/plain":["1    1038\n","2     339\n","4     209\n","3     174\n","5      55\n","0      13\n","Name: possible_polities, dtype: int64"]},"execution_count":96,"metadata":{},"output_type":"execute_result"}],"source":["only_toponyms_sequences_dtf.possible_polities.apply(len).value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["only_toponyms_sequences_dtf[only_toponyms_sequences_dtf.possible_polities.apply(len)>4].head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","## Linking valid statuswords sequences to their polity "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def link_entity_by_typology(dtf_row, polities_dtf):\n","    possible_typologies = statusword_to_typology_dict.get(dtf_row.statusword.text.lower())\n","\n","    if possible_typologies is None:\n","        print(\"WARNING: statusword without corresponding typology: |\"+dtf_row.statusword.text.lower()+\"|\")\n","        return []\n","\n","    possible_polities = [\n","        polities_dtf.loc[(polities_dtf.typology==typology) & polities_dtf.toponym.apply(lambda t: dtf_row.sequence_toponym.text == t)]\n","        for typology in possible_typologies\n","    ]\n","    possible_polities = [dtf for dtf in possible_polities if dtf.shape[0]>0]\n","    return possible_polities\n","\n","def count_nb_matching_tokens(sequence_dtf_row, tokenized_toponym_texts):\n","    sequence_dtf_row_tokens_texts = [t.text for t in sequence_dtf_row.toponym_tokens_sequence]\n","    nb_matching_tokens = sum([\n","        word in sequence_dtf_row_tokens_texts[-(nb_successors+1):]\n","        for word in tokenized_toponym_texts\n","    ])\n","    return nb_matching_tokens\n","\n","def link_entity_by_hdstag(dtf_row, polities_dtf, statusword_to_hdstag_dict):\n","    \"\"\"\n","        # find possible polities: take polities that have matching hds_tag AND an exact match between the searched toponym and th sequence's identified toponym\n","\n","    replacement proposition:\n","    - tokenize polities_dtf canonic_title\n","    - computing toponym matching score\n","\n","    toponym matching score:\n","    - nb_matching_tokens= nb of polities_dtf.toponym_tokens present in sequence_tokens\n","    - all_tokens_matched: whether all tokens of the polities_dtf.toponym_tokens are in the sequence_tokens \n","    - hds_tag_score: score inversely proportional to the rank an hds_tag has in the ordering (rank 0 -> highest score)\n","\n","    ranking algorithm:\n","    -> order according to following order:\n","        1) all_tokens_matched*nb_matching_tokens\n","        2) hds_tag_score\n","        3) nb_matched_tokens\n","    -> score = 100* all_tokens_matched*nb_matching_tokens +\n","                10 * hds_tag_score + \n","                nb_matched_tokens\n","    \"\"\"\n","    possible_hdstags = statusword_to_hdstag_dict.get(dtf_row.statusword.text.lower())\n","\n","    if possible_hdstags is None:\n","        print(\"WARNING: statusword without corresponding hdstag: |\"+dtf_row.statusword.text.lower()+\"|\")\n","        return []\n","\n","    possible_polities = [(\n","            i,\n","            polities_dtf.loc[(polities_dtf.hds_tag==hds_tag) &\n","            polities_dtf.tokenized_toponym_texts.apply(lambda tokens:\n","                any([dtf_row.sequence_toponym.text == t for t in tokens])\n","            )].copy()\n","        )for i,hds_tag in enumerate(possible_hdstags)\n","    ]\n","    for i,dtf in possible_polities:\n","        dtf[\"possibility_hds_tag_rank\"] = i \n","    possible_polities_dtf = pd.concat([dtf for i,dtf in possible_polities])\n","    possible_polities_dtf[\"nb_matching_tokens\"] = possible_polities_dtf.tokenized_toponym_texts.apply(lambda ttt: count_nb_matching_tokens(dtf_row, ttt))\n","    possible_polities_dtf[\"possible_polity_score\"] = \\\n","        100* (possible_polities_dtf.tokenized_toponym_texts.apply(len)==possible_polities_dtf[\"nb_matching_tokens\"]) * possible_polities_dtf[\"nb_matching_tokens\"] + \\\n","        10* (possible_polities_dtf[\"possibility_hds_tag_rank\"].max() - possible_polities_dtf[\"possibility_hds_tag_rank\"])+ \\\n","        possible_polities_dtf[\"nb_matching_tokens\"]\n","    possible_polities_dtf = possible_polities_dtf.sort_values(by ='possible_polity_score', ascending = False)\n","\n","    return possible_polities_dtf\n","\n","def link_statuswords_toponyms_sequences(dtf, polities_dtf, statusword_to_hdstag_dict):\n","    \"\"\"\n","    takes a dtf (valid_sequences_dtf) coming from validate_statuswords_toponyms_sequences()\n","    + adds columns\n","        - possible_polities\n","        - possible_polities_min_rank\n","        - linked_polity_id, linked_hds_tag, linked_toponym\n","    \"\"\"\n","    dtf[\"possible_polities\"] = [\n","        link_entity_by_hdstag(row, polities_dtf, statusword_to_hdstag_dict)\n","        for i, row in tqdm(dtf.iterrows(), total=dtf.shape[0], desc=\"Linking entities by HDS tag\")\n","    ]\n","\n","    #valid_sequences_dtf[\"possible_polities_ranks\"] = valid_sequences_dtf[\"possible_polities\"].apply(lambda pp: [t[0] for t in pp])\n","    dtf[\"possible_polities_min_rank\"] = dtf[\"possible_polities\"].apply(lambda pp_dtf: pp_dtf.possibility_hds_tag_rank.min() if pp_dtf.shape[0]>0 else None)\n","\n","    dtf[\"linked_polity_id\"] = dtf[\"possible_polities\"].apply(lambda pp: pp.iloc[0][\"polity_id\"] if pp.shape[0]>0 else None)\n","    dtf[\"linked_hds_tag\"] = dtf[\"possible_polities\"].apply(lambda pp: pp.iloc[0][\"hds_tag\"]if pp.shape[0]>0 else None)\n","    dtf[\"linked_toponym\"] = dtf[\"possible_polities\"].apply(lambda pp: pp.iloc[0][\"toponym\"]if pp.shape[0]>0 else None)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["link_statuswords_toponyms_sequences(valid_sequences_dtf, polities_dtf, statusword_to_hdstag_dict)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["valid_sequences_dtf.possible_polities.iloc[0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if False:\n","    valid_sequences_dtf[\"possible_polities_by_typology\"] = [\n","        link_entity_by_typology(row, polities_dtf)\n","        for i, row in valid_sequences_dtf.iterrows()\n","    ]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["valid_sequences_dtf[\"possible_polities\"].apply(lambda pp_dtf: pp_dtf.shape[0]).value_counts()\n","valid_sequences_dtf[\"possible_polities_min_rank\"].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["## Exploring linking results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["linked_sequences_human_columns = [\"hds_article_id\", \"statusword\", \"sequence_toponym\", \"sequence\", \"linked_polity_id\", \"linked_hds_tag\", \"linked_toponym\"]\n","\n","valid_sequences_dtf.loc[:,linked_sequences_human_columns]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["linked_sequences_dtf = valid_sequences_dtf.loc[valid_sequences_dtf[\"possible_polities\"].apply(lambda pp: pp.shape[0]>0)].copy()\n","linked_sequences_dtf.loc[:,linked_sequences_human_columns]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["unlinked_sequences_human_columns = [\"hds_article_id\", \"statusword\", \"sequence_toponym\", \"sequence\"]\n","\n","unlinked_sequences_dtf = valid_sequences_dtf.loc[valid_sequences_dtf[\"possible_polities\"].apply(lambda pp: pp.shape[0]==0)].copy()\n","\n","unlinked_sequences_dtf.loc[:,unlinked_sequences_human_columns]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["unlinked_sequences_dtf[\"sequence_text\"] = [\"\".join([t.text_with_ws for t in s]) for s in unlinked_sequences_dtf.sequence]\n","unlinked_sequences_dtf.sequence_text.value_counts().head(20)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["polities_dtf[polities_dtf.typology.apply(lambda t: t is None)].hds_tag.value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["## Annotating linked polities in documents"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def add_annotation_to_document_from_valid_sequences(document, valid_sequences_dtf_rows):\n","    new_annotations = [\n","        Annotation(\n","            row.sequence[0].idx,\n","            row.sequence[-1].idx+len(row.sequence[-1]),\n","            extra_fields={\n","                \"type\": \"polity_id_LOC\",\n","                \"polity_id\": row.linked_polity_id\n","            }\n","        )\n","        for i, row in valid_sequences_dtf_rows.iterrows()\n","    ]\n","    document.annotations = document.annotations + new_annotations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i, row in sampled_articles_dtf.iterrows():\n","    add_annotation_to_document_from_valid_sequences(row.document, valid_sequences_dtf[valid_sequences_dtf.hds_article_id==row.hds_article_id])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# COMPLETING ANNOTATIONS OF MULTI-TOKEN TOPONYMS\n","sampled_articles_dtf.iloc[32,:].hds_article_id\n","\n","valid_sequences_dtf[valid_sequences_dtf.hds_article_id==\"001245\"]\n","dtf_row = valid_sequences_dtf[valid_sequences_dtf.hds_article_id==\"001245\"].iloc[1,:]\n","\n","valid_sequences_dtf[valid_sequences_dtf.hds_article_id==\"001245\"].iloc[0,:].possible_polities\n","\n","#hds_tag = valid_sequences_dtf.loc[valid_sequences_dtf.hds_article_id==\"001245\",[\"hds_tag\"]]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_values = [\n","    valid_sequences_dtf.shape[0],      statusword_tokens_sequences_dtf.shape[0]\n","]\n","truth_sequence = [\n","    valid_sequences_dtf.shape[0]==727, statusword_tokens_sequences_dtf.shape[0]==1456\n","]\n","\n","print(all(truth_sequence))\n","print(test_values)\n","print(truth_sequence)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["valid_sequences_dtf"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["polities_dtf.tokenized_toponym_texts.apply(len).value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["polities_dtf.loc[:,[\"polity_id\", \"tokenized_toponym_texts\"]]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test = linked_sequences_dtf.merge(polities_dtf.loc[:,[\"polity_id\", \"tokenized_toponym_texts\"]], left_on=\"linked_polity_id\", right_on=\"polity_id\", how=\"left\")\n","test.tokenized_toponym_texts.apply(len).value_counts()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"b0ba8461042d62ab06f66d67cc9ca318298fd283d556a98f37e80cf0f5e2bbb1"}}},"nbformat":4,"nbformat_minor":2}
